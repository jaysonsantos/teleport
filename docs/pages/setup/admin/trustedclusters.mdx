---
title: Teleport Trusted Clusters
description: How to configure access and trust between two SSH and Kubernetes environments.
h1: Trusted Clusters
---

Teleport can partition compute infrastructure into multiple clusters. A cluster
is a group of Teleport resources connected to the cluster's Auth Service, which
acts as a certificate authority (CA) for all users and Nodes in the cluster.

Trusted Clusters allow the users of one cluster, the **root cluster**, to
seamlessly SSH into the Nodes of another cluster, the **leaf cluster**, while
remaining authenticated with only a single Auth Service. The leaf cluster can
be running behind a firewall with no TCP ports open to the root cluster.

Uses for Trusted Clusters include:

- Managed service providers (MSP) remotely managing the infrastructure of their clients.
- Device manufacturers remotely maintaining computing appliances deployed on premises.
- Large cloud software vendors managing multiple data centers using a common proxy.

Here is an example of an MSP using Trusted Clusters to obtain access to client clusters:
![MSP Example](../../../img/trusted-clusters/TrustedClusters-MSP.svg)

This guide will explain how to:

- Add and remove Trusted Clusters using CLI commands.
- Enable/disable trust between clusters.
- Establish permission mapping between clusters using Teleport roles.

## Prerequisites

<Tabs>
<TabItem scope={["oss"]} label="Open Source">

- Two running Teleport clusters. For details on how to set up your clusters, see
  one of our [Getting Started](/docs/getting-started) guides.

- The `tctl` admin tool and `tsh` client tool version >= (=teleport.version=).

  ```code
  $ tctl version
  # Teleport v(=teleport.version=) go(=teleport.golang=)

  $ tsh version
  # Teleport v(=teleport.version=) go(=teleport.golang=)
  ```

  See [Installation](/docs/installation.mdx) for details.

- A Teleport Node that is joined to one of your clusters. We will refer to this
  cluster as the **leaf cluster** throughout this guide.

  See [Adding Nodes](../adding-nodes.mdx) for how to launch a Teleport Node in
  your cluster.

</TabItem>
<TabItem
  scope={["enterprise"]} label="Enterprise">

- Two running Teleport clusters. For details on how to set up your clusters, see
  our Enterprise [Getting Started](/docs/enterprise/getting-started) guide.

- The `tctl` admin tool and `tsh` client tool version >= (=teleport.version=),
  which you can download by visiting the
  [customer portal](https://dashboard.gravitational.com/web/login).

  ```code
  $ tctl version
  # Teleport v(=teleport.version=) go(=teleport.golang=)
  
  $ tsh version
  # Teleport v(=teleport.version=) go(=teleport.golang=)
  ```

- A Teleport Node that is joined to one of your clusters. We will refer to this
  cluster as the **leaf cluster** throughout this guide.

  See [Adding Nodes](../adding-nodes.mdx) for how to launch a Teleport Node in
  your cluster.

</TabItem>
<TabItem scope={["cloud"]}
  label="Teleport Cloud">

- A Teleport Cloud account. If you do not have one, visit the
  [sign up page](https://goteleport.com/signup/) to begin your free trial.

- A second Teleport cluster, which will act as the leaf cluster. For details on
how to set up this cluster, see one of our
[Getting Started](/docs/getting-started) guides. 

  As an alternative, you can set up a second Teleport Cloud account.

- The `tctl` admin tool and `tsh` client tool version >= (=cloud.version=).
  To download these tools, visit the [Downloads](/docs/cloud/downloads) page.

  ```code
  $ tctl version
  # Teleport v(=cloud.version=) go(=teleport.golang=)
  
  $ tsh version
  # Teleport v(=cloud.version=) go(=teleport.golang=)
  ```

- A Teleport Node that is joined to one of your clusters. We will refer to this
  cluster as the **leaf cluster** throughout this guide.

  See [Adding Nodes](../adding-nodes.mdx) for how to launch a Teleport Node in
  your cluster.

</TabItem>
</Tabs>

(!docs/pages/includes/permission-warning.mdx!)

## Step 1/4. Establish trust between clusters

Teleport establishes trust between the root cluster and a leaf cluster using
**join tokens**. To register your leaf cluster as a Trusted Cluster, you will first
create a token for it via the root cluster's Auth Service. 

You will then use the Auth Service on the leaf cluster to create a Trusted
Cluster resource. The resource will include the join token, proving to the root
cluster that the leaf cluster is the one you expected to register.

### Create a join token

You can create a join token using the `tctl` tool.

First, log in to the root cluster.

<ScopedBlock scope={["oss", "enterprise"]}>

```code
$ tsh login --user=myuser --proxy=rootcluster.example.com
> Profile URL:        https://rootcluster.example.com:443
  Logged in as:       myuser
  Cluster:            rootcluster.example.com
  Roles:              access, auditor, editor
  Logins:             root
  Kubernetes:         enabled
  Valid until:        2022-04-29 03:07:22 -0400 EDT [valid for 12h0m0s]
  Extensions:         permit-agent-forwarding, permit-port-forwarding, permit-pty
```

</ScopedBlock>
<ScopedBlock scope={["cloud"]}>

```code
$ tsh login --user=myuser --proxy=myrootclustertenant.teleport.sh
> Profile URL:        https://rootcluster.teleport.sh:443
  Logged in as:       myuser
  Cluster:            rootcluster.teleport.sh
  Roles:              access, auditor, editor
  Logins:             root
  Kubernetes:         enabled
  Valid until:        2022-04-29 03:07:22 -0400 EDT [valid for 12h0m0s]
  Extensions:         permit-agent-forwarding, permit-port-forwarding, permit-pty
```

</ScopedBlock>

Execute the following command on your development machine:

```code
# Generates a Trusted Cluster token to allow an inbound connection from a leaf cluster:
$ tctl tokens add --type=trusted_cluster --ttl=15m
The cluster invite token: (=presets.tokens.first=)
This token will expire in 15 minutes

Use this token when defining a trusted cluster resource on a remote cluster.
```

This command generates a Trusted Cluster join token with labels. Every leaf
cluster that uses this token to join the root cluster will inherit the label
`env:demo`.

The token can be used multiple times and has an expiration time of 5 minutes.

Copy the join token for later use. If you need to display your join token again,
run the following command:

```code
$ tctl tokens ls
Token                                                            Type            Labels   Expiry Time (UTC)           
---------------------------------------------------------------- --------------- -------- ---------------------------                      
(=presets.tokens.first=)                                 trusted_cluster env=demo 28 Apr 22 19:19 UTC (4m48s) 
```

<Details title="Revoking join tokens" opened={false}>

You can revoke a join token with the following command:

```code
$ tctl tokens rm (=presets.tokens.first=)
```

</Details>

<Notice
  type="tip"
>

  It's important to note that join tokens are only used to establish a
  connection for the first time. Clusters will exchange certificates and
  won't use tokens to re-establish their connection afterward.

</Notice>

### Define a Trusted Cluster resource

Log out of the root cluster.

```code
$ tsh logout
```

Log in to the leaf cluster:

<ScopedBlock scope={["oss", "enterprise"]}>

```code
$ tsh login --user=myuser --proxy=leafcluster.example.com
```

</ScopedBlock>
<ScopedBlock scope={["cloud"]}>

```code
$ tsh login --user=myuser --proxy=leafcluster.teleport.sh
```

</ScopedBlock>

Ceate a file on your local machine called `trusted_cluster.yaml` with the
following content:

```yaml
# cluster.yaml
kind: trusted_cluster
version: v2
metadata:
  name: rootcluster.example.com
spec:
  enabled: true
  token: ba4825847f0378bcdfe18113c4998498
  tunnel_addr: rootcluster.example.com:11106
  web_proxy_addr: rootcluster.example.com:443
  role_map:
    - remote: "editor"
      local: ["access"]
```

Change the fields of `trusted_cluster.yaml` as follows:

#### `metadata.name`

Use the name of your root cluster, e.g., <ScopedBlock
scope={["oss", "enterprise"]}>`teleport.example.com`</ScopedBlock><ScopedBlock scope="cloud">`mytenant.teleport.sh`</ScopedBlock>.

#### `spec.token`

This is join token you created earlier.

#### `spec.tunnel_addr`

This is the reverse tunnel address of the Proxy Service in the root cluster. Run
the following command to retrieve the port used for reverse tunneling:

<ScopedBlock scope={["oss", "enterprise"]}>

```code
$ PROXY=rootcluster.example.com
$ curl https://${PROXY?}/webapi/ping | jq 'if .proxy.tls_routing_enabled == true then .proxy.ssh.public_addr else .proxy.ssh.ssh_tunnel_public_addr end'
```

</ScopedBlock>
<ScopedBlock scope={["cloud"]}>

```code
$ PROXY=rootcluster.teleport.sh
$ curl https://${PROXY?}/webapi/ping | jq 'if .proxy.tls_routing_enabled == true then .proxy.ssh.public_addr else .proxy.ssh.ssh_tunnel_public_addr end'
```

</ScopedBlock>

Use this port and the address of the root cluster's Proxy Service.

#### `web_proxy_addr`

This is the address of the Proxy Service on the root cluster. Obtain this with the
following command:

<ScopedBlock scope={["oss", "enterprise"]}>

```code
$ curl https://teleport.example.com/webapi/ping | jq .proxy.ssh.public_addr
"teleport.example.com:443"
```

</ScopedBlock>
<ScopedBlock scope={["cloud"]}>

```code
$ curl https://mytenant.teleport.sh/webapi/ping | jq .proxy.ssh.public_addr
"mytenant.teleport.sh:443"
```

</ScopedBlock>

#### `spec.role_map`

Leave this as-is. We will show you how the `role_map` field works later in this
guide.

### Create a Trusted Cluster resource

Still logged in to your leaf cluster, create the Trusted Cluster:

```code
$ tctl create trusted_cluster.yaml
```

At this point, users of the root cluster should see the following when they run
`tsh clusters`, where `teleport.example.com` is the name of the root cluster:

<ScopedBlock scope={["oss", "enterprise"]}>

```code
$ tsh clusters
tsh clusters
Cluster Name                                          Status Cluster Type Selected 
----------------------------------------------------- ------ ------------ -------- 
rootcluster.example.com                               online root         *        
leafcluster.example.com                               online leaf                   
```
</ScopedBlock>
<ScopedBlock scope={["cloud"]}>

```code
$ tsh clusters
Cluster Name                                          Status Cluster Type Selected 
----------------------------------------------------- ------ ------------ -------- 
rootcluster.teleport.sh                               online root         *        
leafcluster.teleport.sh                               online leaf                
```
</ScopedBlock>

## Step 2/4. Manage access to your Trusted Cluster

### Apply labels

When you create a Trusted Cluster resource on the leaf cluster, the leaf
cluster's Auth Service sends a request to the root cluster's Proxy Service to
validate the Trusted Cluster. If the request is valid, the root cluster's Auth
Service creates a `remote_cluster` resource to represent the Trusted Cluster. 

By applying labels to the `remote_cluster` resource on the root cluster, you can
manage access to the leaf cluster. It is not possible to manage labels on the
leaf clusterâ€”allowing leaf clusters to propagate their own labels could create a
problem with rogue clusters updating their labels to bad values.

To retrieve a `remote_cluster`, run the following command:

```code
$ tctl get rc

kind: remote_cluster
metadata:
  id: 1651261581522597792
  name: rootcluster.example.com
status:
  connection: online
  last_heartbeat: "2022-04-29T19:45:35.052864534Z"
version: v3
```

Use `tctl` to update the labels on the leaf cluster:

<ScopedBlock scope="cloud">

```code
$ tctl update rc/leafcluster.teleport.sh --set-labels=env=prod

# Cluster leafcluster.teleport.sh has been updated
```

</ScopedBlock>
<ScopedBlock scope={["oss", "enterprise"]}>

```code
$ tctl update rc/leafcluster.example.com --set-labels=env=prod

# Cluster leafcluster.example.com has been updated
```

</ScopedBlock>

Confirm that the updated labels have been set:

```code
$ tctl get rc

$ sudo tctl get rc
kind: remote_cluster
metadata:
  id: 1651262381521336026
  labels:
    env: prod
  name: rootcluster.example.com
status:
  connection: online
  last_heartbeat: "2022-04-29T19:55:35.053054594Z"
version: v3
```

At this point, the `tctl get rc` command may return an empty result. This is
because you added a label to the cluster. If a cluster has a label, a user's
role must have explicit permission to access clusters with that label before
the user can fetch the cluster's information via `tctl`.

Create a role that allow access to all Trusted Cluster labels by adding the
following content to a file called `prod-cluster-access.yaml`:

```yaml
kind: role
metadata:
  name: prod-cluster-access
spec:
  allow:
    cluster_labels:
      'env': 'prod'
version: v5
```

Create the role:

```code
$ tctl create prod-cluster-access.yaml
role 'prod-cluster-access' has been created
```

Next, retrieve your user's role definition and write it to a file called
`user.yaml`.

```code
$ tctl get user/${USERNAME} > user.yaml
```

Make the following change to `user.yaml`:

```diff
 spec:
   roles:
   - editor
   - access
+  - prod-cluster-access
   status:
     is_locked: false
     lock_expires: "0001-01-01T00:00:00Z"
```

Update your user:

```code
$ tctl create -f user.yaml
```

When you log out of the cluster and log in again, you should see the
`remote_cluster` you just labeled.

### Use role mappings

When a leaf cluster establishes trust with a root cluster, it needs a way to
configure which users from the root cluster can access the leaf cluster and what
permissions should they have. Teleport enables you to limit access to Trusted
Clusters by mapping roles to cluster labels.

When creating a `trusted_cluster` resource, the administrator of the leaf
cluster must define how roles from the root cluster map to roles on the leaf
cluster.

The `trusted_cluster` resource you created earlier defines the following role
mappings:

```yaml
  role_map:
  - local:
    - access
    remote: editor
```

With this configuration, users on the root cluster have restricted access to the
leaf cluster. If a user has the `editor` role on the root cluster, the leaf
cluster will grant them the `access` role instead.

<Details title="Role mapping syntax">

### Wildcard characters

In role mappings, wildcard characters match any characters in a string.

For example, if we wanted to let *any* user from the root cluster connect to the
leaf cluster, we can use a wildcard `*` in the `role_map` like this:

```yaml
role_map:
  - remote: "*"
    local: [access]
```

In this example, we are mapping any roles on the root cluster that begin with
`cluster-` to the role `clusteradmin` on the leaf cluster.

```yaml
role_map:
   - remote: 'cluster-*'
     local: [clusteradmin]
```

### Regular expressions

You can also use regular expressions to map user roles from one cluster to
another. Our regular expression syntax enables you to use capture groups to
reference part of an remote role name that matches a regular expression in the
corresponding local role:

```yaml
  # In this example, remote users with a remote role called 'remote-one' will be
  # mapped to a local role called 'local-one', and `remote-two` becomes `local-two`, etc:
  - remote: "^remote-(.*)$"
    local: [local-$1]
```

Regular expression matching is activated only when the expression starts
with `^` and ends with `$`.

Regular expressions use Google's re2 syntax, which you can read about in the re2 [syntax guide](https://github.com/google/re2/wiki/Syntax).

</Details>

<Details title="Sharing user traits between Trusted Clusters">

You can share user SSH logins, Kubernetes users/groups, and database users/names between Trusted Clusters.

Suppose you have a root cluster with a role named `root` and the following
allow rules:

```yaml
logins: ["root"]
kubernetes_groups: ["system:masters"]
kubernetes_users: ["alice"]
db_users: ["postgres"]
db_names: ["dev", "metrics"]
```

When setting up the Trusted Cluster relationship, the leaf cluster can choose
to map this `root` cluster role to its own `admin` role:

```yaml
role_map:
- remote: "root"
  local: ["admin"]
```

The role `admin` of the leaf cluster can now be set up to use the root cluster's
role logins, Kubernetes groups and other traits using the following variables:

```yaml
logins: ["{{internal.logins}}"]
kubernetes_groups: ["{{internal.kubernetes_groups}}"]
kubernetes_users: ["{{internal.kubernetes_users}}"]
db_users: ["{{internal.db_users}}"]
db_names: ["{{internal.db_names}}"]
```

User traits that come from the identity provider (such as OIDC claims or SAML
attributes) are also passed to the leaf clusters and can be access in the role
templates using `external` variable prefix:

```yaml
logins: ["{{internal.logins}}", "{{external.logins_from_okta}}"]
node_labels:
  env: "{{external.env_from_okta}}"
```

</Details>

<Details title="Trusted Cluster UI" scopeOnly opened scope={["cloud", "enterprise"]}>

You can easily configure leaf nodes using the Teleport Web UI.

Here is an example of creating trust between a leaf and a root node.
![Tunnels](../../../img/trusted-clusters/setting-up-trust.png)
</Details>

<Details title="Updating your Trusted Cluster role mapping">

To update the role map for a Trusted Cluster, run the following commands on the
leaf cluster.

First, remove the cluster:

```code
$ tctl rm tc/root-cluster
```

When this is complete, we can re-create the cluster:

```code
$ tctl create root-user-updated-role.yaml
```

</Details>

{/* TODO: Commands to try out the role mapping */}

## Step 3/4. Access a Node in your remote cluster

Once Trusted Clusters are set up, an admin from the root cluster can see and
access the leaf cluster:

```code
# Log into the root cluster:
$ tsh --proxy=root.example.com login admin
```

Once a connection has been established, it's easy to switch from the root cluster.
![Teleport Cluster Page](../../../img/trusted-clusters/teleport-trusted-cluster.png)

```code
# See the list of available clusters
$ tsh clusters

# Cluster Name   Status
# ------------   ------
# root           online
# leaf           online
```

```code
# See the list of machines (nodes) behind the leaf cluster:
$ tsh ls --cluster=leaf

# Node Name Node ID            Address        Labels
# --------- ------------------ -------------- -----------
# db1.leaf  cf7cc5cd-935e-46f1 10.0.5.2:3022  role=db-leader
# db2.leaf  3879d133-fe81-3212 10.0.5.3:3022  role=db-follower
```

```code
# SSH into any node in "leaf":
$ tsh ssh --cluster=leaf user@db1.leaf
```

<Admonition
  type="tip"
  title="Note"
>
  Trusted Clusters work only one way. In the example above, users from "leaf"
  cannot see or connect to the nodes in "root".
</Admonition>

## Step 4/4. Disable trust between your clusters

To temporarily disable trust between clusters, i.e. to disconnect the "leaf"
cluster from "root", edit the YAML definition of the `trusted_cluster` resource
and set `enabled` to "false", then update it:

```code
$ tctl create --force cluster.yaml
```

### Remove a leaf cluster relationship from both sides

Once established, to fully remove a trust relationship between two clusters, do
the following:

- Remove the relationship from the leaf cluster: `tctl rm tc/root.example.com` (`tc` = Trusted Cluster)
- Remove the relationship from the root cluster: `tctl rm rc/leaf.example.com` (`rc` = remote cluster)

### Remove a leaf cluster relationship from the root

Remove the relationship from the root cluster: `tctl rm rc/leaf.example.com`.

<Admonition type="note">
  The `leaf.example.com` cluster will continue to try and ping the root cluster,
  but will not be able to connect. To re-establish the Trusted Cluster relationship,
  the Trusted Cluster has to be created again from the leaf cluster.
</Admonition>

### Remove a leaf cluster relationship from the leaf

Remove the relationship from the leaf cluster: `tctl rm tc/root.example.com`.

## Troubleshooting

<Tabs>
<TabItem scope={["oss", "enterprise"]} label="Self-Hosted">
There are three common types of problems Teleport administrators can run into when configuring
trust between two clusters:

- **HTTPS configuration**: when the root cluster uses a self-signed or invalid HTTPS certificate.
- **Connectivity problems**: when a leaf cluster does not show up in the output
  of `tsh clusters` on the root cluster.
- **Access problems**: when users from the root cluster get "access denied" error messages
  trying to connect to nodes on the leaf cluster.

### HTTPS configuration

If the `web_proxy_addr` endpoint of the root cluster uses a self-signed or
invalid HTTPS certificate, you will get an error: "the trusted cluster uses
misconfigured HTTP/TLS certificate". For ease of testing, the `teleport` daemon
on the leaf cluster can be started with the `--insecure` CLI flag to accept
self-signed certificates. Make sure to configure HTTPS properly and remove the
insecure flag for production use.

### Connectivity problems

To troubleshoot connectivity problems, enable verbose output for the Auth
Servers on both clusters. Usually this can be done by adding `--debug` flag to
`teleport start --debug`. You can also do this by updating the configuration
file for both Auth Servers:

```yaml
# Snippet from /etc/teleport.yaml
teleport:
  log:
    output: stderr
    severity: DEBUG
```

On systemd-based distributions, you can watch the log output via:

```code
$ journalctl -fu teleport
```

Most of the time you will find out that either a join token is
mismatched/expired, or the network addresses for `tunnel_addr` or
`web_proxy_addr` cannot be reached due to pre-existing firewall rules or
how your network security groups are configured on AWS.

### Access problems

Troubleshooting access denied messages can be challenging. A Teleport administrator
should check to see the following:

- Which roles a user is assigned on the root cluster when they retrieve their SSH
  certificate via `tsh login`. You can inspect the retrieved certificate with the
  `tsh status` command on the client-side.
- Which roles a user is assigned on the leaf cluster when the role mapping takes
  place. The role mapping result is reflected in the Teleport audit log. By
  default, it is stored in `/var/lib/teleport/log` on the Auth Server of a
  cluster. Check the audit log messages on both clusters to get answers for the
  questions above. 
</TabItem>
<TabItem scope={["cloud"]} label="Teleport Cloud">
Troubleshooting "access denied" messages can be challenging. A Teleport administrator
should check to see the following:

- Which roles a user is assigned on the root cluster when they retrieve their SSH
  certificate via `tsh login`. You can inspect the retrieved certificate with the
  `tsh status` command on the client-side.
- Which roles a user is assigned on the leaf cluster when the role mapping takes
  place. The role mapping result is reflected in the Teleport audit log, which
  you can access via the Teleport Web UI.
</TabItem>
</Tabs>

## Further reading
- Read more about how Trusted Clusters fit into Teleport's overall architecture:
  [Architecture Introduction](../../architecture/overview.mdx).

